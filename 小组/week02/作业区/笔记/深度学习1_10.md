# 深度学习
## 一. 何为深度学习
**定义:**

&emsp; &emsp; **深度学习是机器学习的一个子领域，受人脑的神经结构启发**

其中类比生物的神经网络而产生的结构叫做**人工神经网络(Artificial Nerual Network [ANN] AKA net, nerual net, model)**


## 二. 人工神经网络的基本结构
**人工神经网络**包括以下四个**基本结构**：
- **神经元(neurons)**
- 由一组神经元构成的**层(layers)**，包括**输入层(input layers)，输出层(output layers)，隐藏层(hidden layers)**，不同层会对数据进行不同的变换
- 位于ANN内的**非输入与输出层**称为**隐藏层(hidden layers)**
- 如果一个人工神经网络存在不止一层隐藏层，则称其为**深度神经网络(deep ANN)**

其中，从输入层经过隐藏层最终到达输出层的过程叫做**前向传播(forward pass)**

输入层中节点的数量意味着传入数据的**特征数**或者**维度**
输出层中节点的数量意味着得到的**标签数**

例如：输入数据维小鼠的高度与重量，标签为健康与超重

## 三. 层
在人工神经网络中，我们初步学习了层的概念

除了分为输入层，输出层，以及隐藏层，层还可以分为以下几类：

- 全链接层(fully connected layers)
- 卷积层(convolutional layers)
- 池化层(pooling layers)
- 循环层(recurrent layers)
- 归一化层(normalization layers)

其中 卷积层和池化层主要用于**卷积神经网络**，循环从主要用于自然语言处理中

对于非输入层与输出层，一层的其中一个节点都会连接至下一层的每一个节点，并且也会被上一层的所有节点连接

---
#### 层的权重
每两个连接的神经元都会有一个相应得权重，每一个权重代表了**两个神经元的关系紧密程度**
该层中获得的**加权和**会传入**激活函数(activation function)**，一种非线性函数(例如: Sigmoid, ReLU, Softmax, Tanh等等)

即
$$neurons\space outputs= actiavtion\space function(weighted\space sum)$$

#### 前向传播
每次从一个神经元得到一个输出值，这个输出会再次作为下一个神经元输入值，如此直至到达输出层
如此，从数据从输入层到输出层的一个完整的过程叫做**前向传播**

## 四. 激活函数
**定义：**

&emsp; &emsp; **在人工神经网络中，激活函数是将节点的输入映射到相应输出的函数**

激活函数通常将加权和转化成一个属于某个区间的值

#### 激活函数做了什么
**引入 - Sigmoid函数**
$$
sigmoid(x) = \frac{e^x}{e^x + 1}
$$

$$
对于大多数正输入值, sigmoid(x)\rightarrow 1
\newline\space\newline
对于大多数负输入值, sigmoid(x)\rightarrow 0
\newline\space\newline
对于x = 0, sigmoid(x) = 0.5
\newline\space\newline
对于x \rightarrow 0, sigmoid(x) 位于 0与1之间
$$

通过对sigmoid激活函数的理解，我们可以将激活函数类比为生物大脑中神经元根据不同的刺激从而激活

---
**ReLU激活函数**

$$
ReLU(x) = max(0, x)
\newline\space\newline
or
\newline\space\newline
ReLU(x) = \begin{cases}
0 & x = 0\\
x & x \leq  0
\end{cases}
$$

从函数表达式中我们能够看出，对于越大的正输入值，得到的输出也会越大，相当于神经元越活跃


### 为什么使用激活函数
我们知道，激活函数都是非线性的，而神经元间通过权重相连，无论怎样加权求和，最后的结果也是线性的，而深度学习中通常解决的问题都远比线性问题要复杂的多

---
#### 补充:


$证明 ReLU不线性$
$$
f(x) = ReLU(x) \space\space\space \exist a < 0
\newline\space\newline
f(-1a) = max(0, -1a) > 0
\newline\space\newline
(-1)f(a) = (-1)max(0, a) = 0
\newline\space\newline
\therefore f(-1a) \neq (-1)f(a)
\newline\space\newline
故ReLU非线性
$$

## 五. 模型训练
**训练的目的**：
- 优化权），使得模型能够拟合数据集
- 不断迭代直至达到最优解

**训练的过程**
- 随机生成权重
- 构建**损失函数(loss Function)**, 均方误差(MSE)是常用的损失函数
- 选择**优化器(optimizer/ optimization algorithm)**, 随机梯度下降(SGD)是常用的优化器
- 前向传播，得到模型输出
- 模型输出与数据集标签进行处理，得到损失值
- 反向传播，对损失函数求梯度利用优化器更新模型参数，完成一次 **迭代/历元(epoch)**
- 不断迭代，直至达到最优解

**损失函数的梯度(gradient of loss function)**

损失函数的梯度即对损失函数求需要更新的权重的偏导
$$
\nabla Loss(\mathbf{w}) = \frac{\partial Loss}{\partial \mathbf{w}}
$$

**学习率(learning rate)**

学习率是一个权重更新的参数，与损失函数梯度相乘，决定每次参数更新的步长

学习率的范围通常为[0.01, 0.00001]

学习率设置的过高会导致步长过长，最终使得权重无法收敛甚至发散，相反设置过低会导致收敛过慢

**更新权重**
$$
\mathbf{w}^{(k)} = \mathbf{w}^{(k-1)} - lr * \nabla_{\mathbf{w}}Loss\mathbf({w})
$$

## 六. 数据集
数据集(dataset)可划分为3类

| 数据集 | 是否更新权重 | 描述 |
| --- | --- | --- |
| 训练集 | 是 | 用于模型训练，帮助模型拟合数据并同时保证其泛化能力 |
| 验证集 | 否 | 评估训练中的模型的泛化能力，帮助调整模型超参数 |
| 测试集 | 否 | 在训练好的模型投入实际生产前评估其最终泛化能力 |